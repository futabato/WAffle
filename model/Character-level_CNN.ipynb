{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1D-conv",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvmTLbfe0xlX"
      },
      "source": [
        "# 概要\r\n",
        "\r\n",
        "Character-LevelのCNNでWAFを作る。  \r\n",
        "論文の内容をKerasで実装してみる。  \r\n",
        "原著論文はこちら http://iyatomi-lab.info/sites/default/files/user/CSPA2018%20Proceedings_ito.pdf \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1dsexYs1IqJ"
      },
      "source": [
        "## データの準備\r\n",
        "\r\n",
        "inputはURL decode -> Unicode encodeしたもの"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s-l1JL-3hJp"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import roc_curve, auc\r\n",
        "from tensorflow.keras.metrics import Precision, Recall\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.callbacks import TensorBoard\r\n",
        "import keras\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, Flatten, Conv1D, MaxPool1D, Add, Reshape, normalization, Concatenate, merge, GlobalMaxPooling1D\r\n",
        "from keras.utils import plot_model, to_categorical, np_utils\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDxvUVCOPpLC"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "\r\n",
        "try:\r\n",
        "  # %tensorflow_version only exists in Colab.\r\n",
        "  %tensorflow_version 2.x\r\n",
        "except Exception:\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpnOO6KAPtkN"
      },
      "source": [
        "# Load the TensorBoard notebook extension\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NzRURP9uQ9T"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\", 1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIJK0lixnQNM"
      },
      "source": [
        "%cd /content/drive/MyDrive/WAffle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZqaE3AznsXR"
      },
      "source": [
        "df = pd.read_csv('Dataset/cisc_database/csic_database.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKg3lQiIXNiE"
      },
      "source": [
        "df = df.rename(columns={'Unnamed: 0':'Target'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoztO_ZdrpiQ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRoU-S2whGo-"
      },
      "source": [
        "### 前処理\r\n",
        "\r\n",
        "目標: 'Target', 'URL'のみのDataFrameにする。\r\n",
        "DataFrameをtraining, validation, testに6:2:2で分割し、URLをInput、Targetをlabelとなるようにしていく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaEXeyoNp-SE"
      },
      "source": [
        "df = df[['Target', 'URL']]\r\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eveT8z_hP3Q"
      },
      "source": [
        "# Targetカラムの変更。\r\n",
        "# if (df['Target'] == Normal){0} else {1}\r\n",
        "\r\n",
        "# Normalを0, それ以外は1に置換。lossにはbinary_crossentropyを使う。\r\n",
        "df['Target'] = df['Target'].apply(lambda x:0 if str(x) == 'Normal' else 1)\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbg0-M1TZKQe"
      },
      "source": [
        "# URLの末尾にあるHTTP 1.1という文字列を消す\r\n",
        "df['URL'] = df['URL'].str[:-8]\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Df_tee7-Fw"
      },
      "source": [
        "# Training : Validation : Test = 7.5 : 1.5 : 1.0\r\n",
        "\r\n",
        "train_ratio = 0.75\r\n",
        "validation_ratio = 0.15\r\n",
        "test_ratio = 0.10\r\n",
        "\r\n",
        "# train is now 75% of the entire data set\r\n",
        "# the _junk suffix means that we drop that variable completely\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['URL'], df['Target'], test_size=1 - train_ratio, random_state=42)\r\n",
        "\r\n",
        "# test is now 10% of the initial data set\r\n",
        "# validation is now 15% of the initial data set\r\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42) \r\n",
        "\r\n",
        "print(X_train, X_val, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUrwXAdy-2L"
      },
      "source": [
        "print('X_train shape', X_train.shape)\r\n",
        "print('y_train shape', y_train.shape)\r\n",
        "\r\n",
        "print('X_val shape', X_val.shape)\r\n",
        "print('y_val shape', y_val.shape)\r\n",
        "\r\n",
        "print('X_test shape', X_test.shape)\r\n",
        "print('y_test shape', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw2ERjY4Zqoh"
      },
      "source": [
        "test_data = pd.concat([X_test, y_test], axis=1)\r\n",
        "test_data = pd.DataFrame(test_data)\r\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j18SJ0Ssb3rk"
      },
      "source": [
        "# testデータをcsvファイルにするで\r\n",
        "test_data.to_csv('Dataset/cisc_database/test_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTtR_qwyU4Eu"
      },
      "source": [
        "#いつか使う URL decode\r\n",
        "import urllib.parse\r\n",
        "def url_decode(encoded_URL):\r\n",
        "  return urllib.parse.unquote(encoded_URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXQxLqKmLh61"
      },
      "source": [
        "def load_data(urls, max_length=1000):\r\n",
        "    urls = [s.lower() for s in urls]\r\n",
        "    url_list = []\r\n",
        "    for url in urls:\r\n",
        "        # url decode\r\n",
        "        decoded_url = url_decode(url)\r\n",
        "        # unicode encode\r\n",
        "        encoded_url = [ord(x) for x in str(decoded_url).strip()]\r\n",
        "        encoded_url = encoded_url[:max_length]\r\n",
        "        url_len = len(encoded_url)\r\n",
        "        if url_len < max_length:\r\n",
        "            # zero padding\r\n",
        "            encoded_url += ([0] * (max_length - url_len))\r\n",
        "        url_list.append((encoded_url))\r\n",
        "    # convert to numpy array\r\n",
        "    url_list = np.array(url_list)\r\n",
        "    return url_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW_3RamYLphx"
      },
      "source": [
        "train_data = load_data(X_train)\r\n",
        "val_data = load_data(X_val)\r\n",
        "test_data = load_data(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWKde8CaJw91"
      },
      "source": [
        "print(train_data)\r\n",
        "print(val_data)\r\n",
        "print(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htkiNenQJK9Q"
      },
      "source": [
        "def create_label(labels):\r\n",
        "  class_list = [x for x in labels]\r\n",
        "  classes = np.array(class_list)\r\n",
        "  return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpCq1g3tolPd"
      },
      "source": [
        "train_classes = create_label(y_train)\r\n",
        "val_classes = create_label(y_val)\r\n",
        "test_classes = create_label(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4CZChOrYT_D"
      },
      "source": [
        "print(train_classes)\r\n",
        "print(val_classes)\r\n",
        "print(test_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P77ahKXc0lo9"
      },
      "source": [
        "## アーキテクチャ\r\n",
        "\r\n",
        "1. Input(1000 characters)\r\n",
        "2. Embedding(128demensions)\r\n",
        "3. Conv(kernelsize = K, filter_num = 64) -> RELU\r\n",
        "4. Max Pooling(kernelsize = K)\r\n",
        "5. Conv(kernelsize = K, filter_num = 64) -> RELU\r\n",
        "6. Max Pooling(kernel_size = size of Conv 5's output)\r\n",
        "7. concat output & reshape into 256 length vector\r\n",
        "8. FCN(64 units) -> RELU\r\n",
        "9. Batch normalize\r\n",
        "10. Dropout(0.5)\r\n",
        "11. FCN(1 unit) -> Sigmoid\r\n",
        "\r\n",
        "(categorical_crossentropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TyhzcjsL7wd"
      },
      "source": [
        "def create_model(input_max_size, embedding_size, kernel_sizes, dropout):\r\n",
        "\r\n",
        "  # Input Layer\r\n",
        "  # URLdecode -> Unicode encode -> npumpy.darrayに変換されたURLをInputとして与える。\r\n",
        "  inputs = Input(shape=(input_max_size,), name='URL_input')\r\n",
        "\r\n",
        "  # Embedding Layer\r\n",
        "  x = Embedding(0xffff, embedding_size, name='Embedding')(inputs)\r\n",
        "  x = Reshape((input_max_size, embedding_size), name='Reshape_into_128_legnth_vector')(x)\r\n",
        "\r\n",
        "  # Convolution Layers\r\n",
        "  convolution_output = []\r\n",
        "\r\n",
        "  for kernel_size in kernel_sizes:\r\n",
        "    conv1 = Conv1D(64, kernel_size, activation='relu', padding='same', strides=1)(x)\r\n",
        "    pool1 = MaxPool1D(pool_size=kernel_size, padding='same', strides=1)(conv1)\r\n",
        "    conv2 = Conv1D(64, kernel_size, activation='relu', padding='same', strides=1)(pool1)\r\n",
        "    pool2 = GlobalMaxPooling1D()(conv2)\r\n",
        "    convolution_output.append(pool2)\r\n",
        "\r\n",
        "  # concat output\r\n",
        "  x = Concatenate(name='Concat_the_outputs')(convolution_output)\r\n",
        "\r\n",
        "  # reshape into 256 length vector\r\n",
        "  x = Reshape((256, ), name='Reshape_into_256_length_vector')(x)\r\n",
        "\r\n",
        "  # Fully Connected Layers\r\n",
        "  x = Dense(64, activation='relu', name='FullyConnectedLayer')(x)\r\n",
        "\r\n",
        "  # Batch Normalization\r\n",
        "  x = normalization.BatchNormalization()(x)\r\n",
        "\r\n",
        "  # DropOut\r\n",
        "  x = Dropout(dropout)(x)\r\n",
        "\r\n",
        "  # Fully Connected Layers\r\n",
        "  predictions = Dense(1, activation='sigmoid', name='Prediction')(x)\r\n",
        "\r\n",
        "  model = Model(inputs=inputs, outputs=predictions, name='Character-level_CNN')\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVK2iZ7WV8BL"
      },
      "source": [
        "# config\r\n",
        "learning_rate = 0.001\r\n",
        "epochs = 200\r\n",
        "\r\n",
        "input_max_size = 1000\r\n",
        "embedding_size = 128\r\n",
        "kernel_sizes = [4,5,6,7]\r\n",
        "dropout = 0.5\r\n",
        "\r\n",
        "loss = 'binary_crossentropy'\r\n",
        "optimizer = 'adam'\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "model_filepath = 'model_dir/model'\r\n",
        "checkpoint_filepath = '/logs/checkpoint'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdMKFjA2Ab2H"
      },
      "source": [
        "def train(learning_rate, input_max_size, embedding_size, kernel_sizes, dropout, loss, optimizer, train_data, train_classes, epochs, batch_size, val_data, val_classes, model_filepath):\r\n",
        "\r\n",
        "    # 学習率を少しずつ下げるようにする\r\n",
        "    start = learning_rate\r\n",
        "    stop = learning_rate * 0.01\r\n",
        "    learning_rates = np.linspace(start, stop, epochs)\r\n",
        "\r\n",
        "    # モデル作成\r\n",
        "    model = create_model(input_max_size, embedding_size, kernel_sizes, dropout)\r\n",
        "    optimizer = optimizers.Adam(lr=learning_rate)\r\n",
        "    model.compile(loss=loss,\r\n",
        "                  optimizer=optimizer,\r\n",
        "                  metrics=['accuracy', Precision(), Recall(), 'binary_accuracy', 'categorical_accuracy'])\r\n",
        "    \r\n",
        "    tf_callback = TensorBoard(log_dir=\"logs\", histogram_freq=1)\r\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "        filepath=checkpoint_filepath,\r\n",
        "        save_weights_only=True,\r\n",
        "        monitor='val_accuracy',\r\n",
        "        mode='max',\r\n",
        "        save_best_only=True\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "    # 学習\r\n",
        "    model.fit(train_data, train_classes,\r\n",
        "              epochs=epochs,\r\n",
        "              batch_size=batch_size,\r\n",
        "              verbose=2,\r\n",
        "              validation_data=(val_data, val_classes),\r\n",
        "              callbacks=[model_checkpoint_callback]\r\n",
        "              )\r\n",
        "\r\n",
        "    model.save(model_filepath + '.h5')\r\n",
        "    model.save_weights(model_filepath + '_weight.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBGjddN6BIuZ"
      },
      "source": [
        "train(learning_rate, input_max_size, embedding_size, kernel_sizes, dropout, loss, optimizer, train_data, train_classes, epochs, batch_size, val_data, val_classes, model_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLAu_1PYLAnI"
      },
      "source": [
        "model_filepath = 'model_dir/model'\r\n",
        "model = load_model(model_filepath+'.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTcdqhq3SHP5"
      },
      "source": [
        "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_data, test_classes, verbose=2)\r\n",
        "\r\n",
        "print(\"test_loss: \", test_loss)\r\n",
        "print(\"test_accuracy: \", test_accuracy)\r\n",
        "print(\"test_precision: \", test_precision)\r\n",
        "print(\"test_recall: \", test_recall)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}